*****************************************************************************************************************+
Gradient Descent

#### Convergence Formulae

- θ = θ + (α * θ_gradient) => Convergence formulae - used to converge to the minima , 
    - θ is the parameter b/m
    - α is the learning rate
- partial differentiation of cost function (J(θ)) = gradient/slope of b/m
- (as partial differentiation takes one variable at a time others constant thats what we need)
- slope of curve of m wrt J(Θ) when b constant and slope of curve of b wrt J(θ) when m constant (Partial Differentiatio)
- m and J(θ) = 2D curve (we find minima for it ie - m with minimum cost)
- b and J(θ) = 2D curve
together m,b,J(θ) = (3D curve with minimum cost m,b)

***********************************************************************************************************


Ridge(L2) and Lasso(L1) (Regularisation 1 and 2), ElasticNet

Ridge: costfunction + lambda(slope)^2   
Lasso: costfunction + lambda|slope|
ElasticNet: costfunction + lambda(slope)^2 +lambda|slope|

lambda: a type of learning rate (like alpha in gradient descnt)
the additional terms with costfunction are error terms to avoid overfitting as linear Regression is a very simple model


*************************************************************************************************************

*Bias and variance

*Bias

Generally, a linear algorithm has a high bias, as it makes them learn fast. The simpler the algorithm,
the higher the bias it has likely to be introduced. Whereas a nonlinear algorithm often has low bias.

Some examples of machine learning algorithms with low bias are Decision Trees, k-Nearest Neighbours and 
Support Vector Machines. At the same time, an algorithm with high bias is Linear Regression, 
Linear Discriminant Analysis and Logistic Regression.

Ways to reduce High Bias:
High bias mainly occurs due to a much simple model. Below are some ways to reduce the high bias:

- Increase the input features as the model is underfitted.
- Decrease the regularization term.
- Use more complex models, such as including some polynomial features.

Variance

Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis 
and Logistic Regression.

Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and 
Support Vector Machines.

*********************************************************************************************************

High bias => Underfitting                
High variance => Overfitting              

- Low variance , Low bias = Good fit    (not least , low)

An optimal balance of bias and variance would never overfit or underfit the model.


**********************************************************************************************************

Filling Missing Values / IMPUTATION

axis=1, axis=0

- lambda() & apply() used when all null to be replace with mean/medaian or mode 
      
      train['Fare']=train['Fare'].apply(lambda x: np.log(x+1))

- only apply() used when filling must be done using a function

def impute_age(cols):
        Age = cols[0]
        Title = cols[1]
        
        if pd.isnull(Age):
            
            if Title==0:
                return 30
            
            elif Title==1:
                return 25
            
            elif Title==2:
                return 34
            
            elif Title==3:
                return 8
            
            else:
                return 43
        
        else:
            return Age


      df['Age']= df[['Age','Title']].apply(impute_age,axis=1) 
********************************************************************
IMPUTATION


1. df[[cols]].appply(function,axis=1)       
2. df[i].fillna(df[i].mode()[0]) or median()[0]/mean()
    - [0] b/c we can hv more than one median or mode
3. sklearn imputer
4. knn imputer or using knn regressor (for numeric columns only)
    - cant use directly for categorical values

*******************************************************************************************************

os.chdir('Path')
os.walk

**************************************************************************************************************************
ENCODING (Label or Onehot or pd.get_dummies)
********************************************************************************************************
                                        (1)
what is converting categorical values to numerical value is called then?
Labelencoding
Why is it used? 
B/c most of the Ml algos are unable to handle categoric data so they must be converted to numeric values

the doubt:why we need to cnvrt datatypes if in the end all are to be converted to numeric?
ans:-we need to have proper datatypes to impute the missing values properly

# Import label encoder 
from sklearn import preprocessing
# label_encoder object knows how to understand word labels. 
                      
                      label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'Country'. 

                      data['Country']= label_encoder.fit_transform(data[‘Country']) 
                      print(data.head())

**********************************************************************************************************
                                       (2)
pd.get_dummies()
OR
Onehotencoder()   both have same use they convert all categories of a categorical variable to individual attributes

But why is it used?
a decision tree can be learned directly from categorical data with no data transformation required
 (this depends on the specific implementation). Many machine learning algorithms, instead, require all the input and output 
 variables to be numeric.
In these cases, one-hot encoding comes in help because it transforms categorical data into numerical; in other words:
it transforms strings into numbers so that we can apply our Machine Learning algorithms without any problems.

But in Titanic it wasn't used but in Chennai it is used why?


Ans : Either one of the two (Labelencoding or Onehotencding) is used depending on situation

************************************************************************************************************

BINNING

Binning => converting numerical values to categorical by creating bins-(ie ranges for them)

************************************************************************************************************

- Pandas Series.str.extract() function is used to extract capture groups in the regex pat as columns in a DataFrame.
For each subject string in the Series, extract groups from the first match of regular expression pattern.

df['Title']=df['Name'].str.extract(' ([A-Za-z]+)\.',expand=False)

OR we can use smthng like df['Cabin'].split(/)  (/,.,)


**********************************************************************

joblib vs pickle

***********************************************************************************

Q: What is  PCA (Principle Component Analysis ) :

ans: 