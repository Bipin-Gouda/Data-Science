{"cells":[{"cell_type":"markdown","metadata":{},"source":["Many begginers here comparing their result with the leaderboard have the impression that they are doing something wrong, but a result of about 77% is a normal one, the problem is to push it a few percents higher.\n","\n","This notebook used the standard dataset and scored 0.811 on the leaderboard that puts it in top 1%, if not take into account the top results based on cheating or that used an extended dataset. \n","\n","Some feature engineering showed here could be interesting for many beginners as am I by myself, so all the critics, sugestions and rocks of any diameter thrown - are very welcome :)\n","\n","Have fun!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:06.182366Z","iopub.status.busy":"2023-02-19T23:53:06.18189Z","iopub.status.idle":"2023-02-19T23:53:07.678442Z","shell.execute_reply":"2023-02-19T23:53:07.676365Z","shell.execute_reply.started":"2023-02-19T23:53:06.182275Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import (SimpleImputer, IterativeImputer)\n","from sklearn.preprocessing import (OneHotEncoder, StandardScaler)\n","from sklearn.model_selection import (GridSearchCV, cross_val_score)\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.cluster import KMeans\n","from imblearn.over_sampling import SMOTE\n","from catboost import CatBoostClassifier"]},{"cell_type":"markdown","metadata":{},"source":["# Load and analyze data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:07.68194Z","iopub.status.busy":"2023-02-19T23:53:07.681445Z","iopub.status.idle":"2023-02-19T23:53:07.730849Z","shell.execute_reply":"2023-02-19T23:53:07.728857Z","shell.execute_reply.started":"2023-02-19T23:53:07.681897Z"},"trusted":true},"outputs":[],"source":["# Load data\n","full_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n","test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n","full_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:07.733472Z","iopub.status.busy":"2023-02-19T23:53:07.733011Z","iopub.status.idle":"2023-02-19T23:53:07.767819Z","shell.execute_reply":"2023-02-19T23:53:07.76716Z","shell.execute_reply.started":"2023-02-19T23:53:07.733441Z"},"trusted":true},"outputs":[],"source":["# Separate test_df PassengerId (will need it for submission)\n","test_pass_id = test_df.pop('PassengerId')\n","\n","# Keep max index that will be used to back split training and test data\n","X_max_index = full_df.shape[0]\n","\n","# Separate features and target\n","y = full_df.Survived\n","\n","df = full_df.drop(['Survived', 'PassengerId'], axis=1)\n","df = pd.concat([df, test_df], axis=0).reset_index(drop=True)\n","\n","df.info()"]},{"cell_type":"markdown","metadata":{},"source":["- Some features need imputation\n","- Cabin column has a lot of missing values, we will use the availble \n","  values to create a new feature and will drop Cabin\n","- We will create the feature Deck level, using the correlation between\n","  Pclass and info deducted from Cabin column. We suppose that the deck \n","  level could take a role in survivability of the people as the lifeboats \n","  were on the top level.\n","- From Name we will keep just the lastname and use it during creation \n","  of Deck_level.\n","- We will create the feature Title, extracting the title from Name column, \n","  supposing that some category of people had priority to embark the lifeboats."]},{"cell_type":"markdown","metadata":{},"source":["# Unprocessed data correlation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:07.770406Z","iopub.status.busy":"2023-02-19T23:53:07.769474Z","iopub.status.idle":"2023-02-19T23:53:07.782082Z","shell.execute_reply":"2023-02-19T23:53:07.780444Z","shell.execute_reply.started":"2023-02-19T23:53:07.770378Z"},"trusted":true},"outputs":[],"source":["full_df.corr()['Survived'].sort_values(ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["- Fare and Pclass have the highest correlation to Survived, it seams higher class (also higher Fare) had priority to embarc the lifeboats.\n","- Pclass has negative correlation because Pclass is numbered 1, 2, 3 (high, medium, low), but results to an invers survivability (class 3 = lower chance to survive, class 1 = higher chance)."]},{"cell_type":"markdown","metadata":{},"source":["# Features' instances"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:07.784083Z","iopub.status.busy":"2023-02-19T23:53:07.78361Z","iopub.status.idle":"2023-02-19T23:53:08.558583Z","shell.execute_reply":"2023-02-19T23:53:08.557492Z","shell.execute_reply.started":"2023-02-19T23:53:07.784051Z"},"trusted":true},"outputs":[],"source":["df.hist(bins=30, figsize=(12, 8))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["- Attributes have different scales\n","- Some features are skewed right, we should check for outliers and normalize data\n","- Fare has values of 0 that looks weird"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.56052Z","iopub.status.busy":"2023-02-19T23:53:08.560014Z","iopub.status.idle":"2023-02-19T23:53:08.566251Z","shell.execute_reply":"2023-02-19T23:53:08.564454Z","shell.execute_reply.started":"2023-02-19T23:53:08.560492Z"},"trusted":true},"outputs":[],"source":["# Zero values in Fare we will consider as an error or outlier and will delete for further imputation\n","df.loc[df.Fare.eq(0), 'Fare'] = np.nan"]},{"cell_type":"markdown","metadata":{},"source":["# Create Lastname"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.568098Z","iopub.status.busy":"2023-02-19T23:53:08.567728Z","iopub.status.idle":"2023-02-19T23:53:08.58662Z","shell.execute_reply":"2023-02-19T23:53:08.584874Z","shell.execute_reply.started":"2023-02-19T23:53:08.568067Z"},"trusted":true},"outputs":[],"source":["df['Lastname'] = df.Name.str.split(', ').str[0]"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-01-11T15:59:25.339772Z","iopub.status.busy":"2023-01-11T15:59:25.336617Z","iopub.status.idle":"2023-01-11T15:59:25.372076Z","shell.execute_reply":"2023-01-11T15:59:25.371041Z","shell.execute_reply.started":"2023-01-11T15:59:25.339713Z"}},"source":["# Create Title"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.589291Z","iopub.status.busy":"2023-02-19T23:53:08.587857Z","iopub.status.idle":"2023-02-19T23:53:08.608693Z","shell.execute_reply":"2023-02-19T23:53:08.606536Z","shell.execute_reply.started":"2023-02-19T23:53:08.589219Z"},"trusted":true},"outputs":[],"source":["# Extracting the Title from Name column\n","df['Title'] = df.Name.str.split(', ').str[1]\n","df['Title'] = df.Title.str.split('.').str[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.612481Z","iopub.status.busy":"2023-02-19T23:53:08.61038Z","iopub.status.idle":"2023-02-19T23:53:08.62333Z","shell.execute_reply":"2023-02-19T23:53:08.622197Z","shell.execute_reply.started":"2023-02-19T23:53:08.612439Z"},"trusted":true},"outputs":[],"source":["# Analyze titles\n","df.Title.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["There are some title with the same meaning that should be joined together and also many unique titles that we will group under the title 'Noble'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.627112Z","iopub.status.busy":"2023-02-19T23:53:08.626764Z","iopub.status.idle":"2023-02-19T23:53:08.643539Z","shell.execute_reply":"2023-02-19T23:53:08.641414Z","shell.execute_reply.started":"2023-02-19T23:53:08.627081Z"},"trusted":true},"outputs":[],"source":["# Analyze the title Mr and the Age\n","df[df.Title.eq('Mr')].Age.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.645085Z","iopub.status.busy":"2023-02-19T23:53:08.644707Z","iopub.status.idle":"2023-02-19T23:53:08.658501Z","shell.execute_reply":"2023-02-19T23:53:08.657532Z","shell.execute_reply.started":"2023-02-19T23:53:08.645052Z"},"trusted":true},"outputs":[],"source":["# Analyze the title Master and the Age\n","df[df.Title.eq('Master')].Age.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Title Mr was used from 11 years old and Master to maximum 15 years old. \n","Master is an antiquated title for an underage male.\n","We will join them together and then split again at age 15 to have a clean delimeter."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.660385Z","iopub.status.busy":"2023-02-19T23:53:08.659933Z","iopub.status.idle":"2023-02-19T23:53:08.676929Z","shell.execute_reply":"2023-02-19T23:53:08.67465Z","shell.execute_reply.started":"2023-02-19T23:53:08.660359Z"},"trusted":true},"outputs":[],"source":["# Grouping the same type titles \n","\n","# We change also Miss to Mrs, but later we will convert back \n","# to Miss just for young females as for now Miss is not \n","# very usefull as it represents a young lady and also \n","# an unmarried adult one of any age\n","females = ['Ms', 'Miss', 'Mlle', 'Mrs', 'Mme']\n","df.loc[df.Title.isin(females), 'Title'] = 'Mrs'\n","\n","males = ['Master', 'Mr']\n","df.loc[(df.Title.isin(males)), 'Title'] = 'Mr'\n","\n","# Change the titles for children to Master and Miss\n","df.loc[(df.Title.eq('Mr') & df.Age.lt(15)), 'Title'] = 'Master'\n","df.loc[(df.Title.eq('Mrs') & df.Age.lt(15)), 'Title'] = 'Miss'\n","\n","# Create noble title\n","df.loc[(~df.Title.isin(females) & ~df.Title.isin(males)), 'Title'] = 'Noble'"]},{"cell_type":"markdown","metadata":{},"source":["# Create Price\n","\n","We should divide the Fare by number of passengers on the same ticket"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.681418Z","iopub.status.busy":"2023-02-19T23:53:08.679883Z","iopub.status.idle":"2023-02-19T23:53:08.710747Z","shell.execute_reply":"2023-02-19T23:53:08.70923Z","shell.execute_reply.started":"2023-02-19T23:53:08.68136Z"},"trusted":true},"outputs":[],"source":["# Analyze Fare by ticket number to be sure that the Fare represents \n","# the full price of the ticket and not the price per person\n","\n","# Split Ticket by series and number\n","df['Ticket_series'] = [i[0] if len(i) > 1 else 0 for i in df.Ticket.str.split()]\n","df['Ticket_nr'] = [i[-1] for i in df.Ticket.str.split()]\n","\n","# Check if Fare min and Fare max of the same ticket number are the same\n","df_fare = df[~df.Fare.isna()]\n","multi_tickets = df_fare.groupby(df_fare.Ticket_nr[df_fare.Ticket_nr.duplicated()])\n","(multi_tickets.Fare.min() != multi_tickets.Fare.max()).sum()"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-02-04T20:10:52.855161Z","iopub.status.busy":"2023-02-04T20:10:52.854685Z","iopub.status.idle":"2023-02-04T20:10:52.865727Z","shell.execute_reply":"2023-02-04T20:10:52.864764Z","shell.execute_reply.started":"2023-02-04T20:10:52.85512Z"}},"source":["There is just 1 ticket where min and max don't corespond, we will ignore it as a mistake"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.714708Z","iopub.status.busy":"2023-02-19T23:53:08.714291Z","iopub.status.idle":"2023-02-19T23:53:08.727789Z","shell.execute_reply":"2023-02-19T23:53:08.726874Z","shell.execute_reply.started":"2023-02-19T23:53:08.714675Z"},"trusted":true},"outputs":[],"source":["# Create a column with the passengers number by ticket \n","ticket_dict = df.groupby('Ticket_nr').Lastname.count().to_dict()\n","df['Passengers_ticket'] = df.Ticket_nr.map(ticket_dict)\n","\n","# Create Price column\n","df['Price'] = (df.Fare / df.Passengers_ticket).round()"]},{"cell_type":"markdown","metadata":{},"source":["# Create Deck\n","\n","This will have the deck letter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.729197Z","iopub.status.busy":"2023-02-19T23:53:08.72879Z","iopub.status.idle":"2023-02-19T23:53:08.741303Z","shell.execute_reply":"2023-02-19T23:53:08.740336Z","shell.execute_reply.started":"2023-02-19T23:53:08.729157Z"},"trusted":true},"outputs":[],"source":["# Extract Deck letter from Cabin column\n","df['Deck'] = df.Cabin.str[0]\n","\n","# Check how many missing values we have at this step\n","df.Deck.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.743123Z","iopub.status.busy":"2023-02-19T23:53:08.742799Z","iopub.status.idle":"2023-02-19T23:53:08.758557Z","shell.execute_reply":"2023-02-19T23:53:08.756918Z","shell.execute_reply.started":"2023-02-19T23:53:08.743098Z"},"trusted":true},"outputs":[],"source":["# Deck distribution by Pclass\n","df.groupby('Pclass').Deck.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.760931Z","iopub.status.busy":"2023-02-19T23:53:08.760506Z","iopub.status.idle":"2023-02-19T23:53:08.772554Z","shell.execute_reply":"2023-02-19T23:53:08.771276Z","shell.execute_reply.started":"2023-02-19T23:53:08.760895Z"},"trusted":true},"outputs":[],"source":["# Deck missing values by Pclass\n","df.loc[df.Deck.isna(), 'Pclass'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["- On the 1st step we will impute the Deck letter based on Ticket_nr, if the same Ticket_nr has already an available \n","  value for Deck in other rows\n","  \n","- On the 2nd step we will impute based on Lastname using the same method as in the first step, but to be sure that \n","  the passengers are not from different families with the same Lastname we will use some filters in the process.\n","\n","- On the 3rd step we will impute based on Pclass, as every Pclass was on separate Deck with some intersections between \n","  (some googling confirms that class-deck distribution corresponds to our Deck distribution by Pclass analysis). \n","  To improve the accuracy we will check also the mean Price for each Pclass-Deck group to determine the Deck. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.774726Z","iopub.status.busy":"2023-02-19T23:53:08.773798Z","iopub.status.idle":"2023-02-19T23:53:08.783729Z","shell.execute_reply":"2023-02-19T23:53:08.78253Z","shell.execute_reply.started":"2023-02-19T23:53:08.774687Z"},"trusted":true},"outputs":[],"source":["# Function for imputing Deck\n","def impute_deck_by(feature):\n","    for pclass in range(1, 4):\n","        # Create a mapping dictionary\n","        map_dic = (df[~df.Deck.isna() & df.Pclass.eq(pclass)]\n","                   .groupby(feature).Deck.unique()\n","                   .apply(list).to_dict())\n","\n","        # Keep just the keys with a single deck to avoid \n","        # the same key on different decks\n","        map_dic = {i:j[0] for i, j in map_dic.items() \n","                   if len(j) == 1}\n","\n","        # Imputing Deck from map_dic\n","        df.loc[df.Deck.isna() & df.Pclass.eq(pclass), \n","               'Deck'] = df[feature].map(map_dic)\n","\n","    # Check how many missing values we have at this step\n","    print(df.Deck.isna().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.786806Z","iopub.status.busy":"2023-02-19T23:53:08.785189Z","iopub.status.idle":"2023-02-19T23:53:08.823493Z","shell.execute_reply":"2023-02-19T23:53:08.822463Z","shell.execute_reply.started":"2023-02-19T23:53:08.786759Z"},"trusted":true},"outputs":[],"source":["impute_deck_by('Ticket_nr')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.825955Z","iopub.status.busy":"2023-02-19T23:53:08.825009Z","iopub.status.idle":"2023-02-19T23:53:08.858422Z","shell.execute_reply":"2023-02-19T23:53:08.857091Z","shell.execute_reply.started":"2023-02-19T23:53:08.825921Z"},"trusted":true},"outputs":[],"source":["impute_deck_by('Lastname')"]},{"cell_type":"markdown","metadata":{},"source":["We have recovered 25 values, not much, but they correspond to reality,\n","the rest we will impute later based on Pclass and Price as mentioned earlier."]},{"cell_type":"markdown","metadata":{},"source":["# Impute Age"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.860669Z","iopub.status.busy":"2023-02-19T23:53:08.860243Z","iopub.status.idle":"2023-02-19T23:53:08.880638Z","shell.execute_reply":"2023-02-19T23:53:08.878973Z","shell.execute_reply.started":"2023-02-19T23:53:08.860629Z"},"trusted":true},"outputs":[],"source":["# List of titles\n","titles = list(df.Title.unique())\n","\n","# Impute median Age by title\n","for title in titles:\n","    df.loc[(df.Age.isna() & df.Title.eq(title)), 'Age'] = df.loc[df.Title.eq(title), 'Age'].median()"]},{"cell_type":"markdown","metadata":{},"source":["# Analyze and impute missing prices\n","\n","We impute prices first as there are less missing values in Price than in Deck and we use them both for imputation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.882908Z","iopub.status.busy":"2023-02-19T23:53:08.882479Z","iopub.status.idle":"2023-02-19T23:53:08.939497Z","shell.execute_reply":"2023-02-19T23:53:08.93857Z","shell.execute_reply.started":"2023-02-19T23:53:08.88286Z"},"trusted":true},"outputs":[],"source":["# Analyze Price by Deck and Pclass\n","df.groupby(['Pclass', 'Deck']).Price.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Very large standard deviation in Pclass 1, Deck B comparing to others, we should analyze this."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.941902Z","iopub.status.busy":"2023-02-19T23:53:08.940639Z","iopub.status.idle":"2023-02-19T23:53:08.948098Z","shell.execute_reply":"2023-02-19T23:53:08.946693Z","shell.execute_reply.started":"2023-02-19T23:53:08.941864Z"},"trusted":true},"outputs":[],"source":["# Cabin T was on the upper deck (google helps), \n","# so we will replace it with A deck as it has just a single value\n","df.loc[df.Deck.eq('T'), 'Deck'] = 'A'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.950674Z","iopub.status.busy":"2023-02-19T23:53:08.949818Z","iopub.status.idle":"2023-02-19T23:53:08.9803Z","shell.execute_reply":"2023-02-19T23:53:08.978379Z","shell.execute_reply.started":"2023-02-19T23:53:08.950639Z"},"trusted":true},"outputs":[],"source":["# Check the cheapest prices for Deck B\n","df[df.Deck.eq('B')].sort_values('Price').head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.982669Z","iopub.status.busy":"2023-02-19T23:53:08.982275Z","iopub.status.idle":"2023-02-19T23:53:08.993778Z","shell.execute_reply":"2023-02-19T23:53:08.992089Z","shell.execute_reply.started":"2023-02-19T23:53:08.982638Z"},"trusted":true},"outputs":[],"source":["# Maybe Mr Carlsson paid just 5 pounds for that 1st class ticket, \n","# but this value is an outlier that we will replace with the next min\n","df.loc[df.Ticket_nr.eq('695'), 'Price'] = 19"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:08.996896Z","iopub.status.busy":"2023-02-19T23:53:08.996233Z","iopub.status.idle":"2023-02-19T23:53:09.028389Z","shell.execute_reply":"2023-02-19T23:53:09.026597Z","shell.execute_reply.started":"2023-02-19T23:53:08.996851Z"},"trusted":true},"outputs":[],"source":["# Check the most expensive prices for Deck B\n","df[df.Deck.eq('B')].sort_values('Price', ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.031525Z","iopub.status.busy":"2023-02-19T23:53:09.031065Z","iopub.status.idle":"2023-02-19T23:53:09.042492Z","shell.execute_reply":"2023-02-19T23:53:09.040877Z","shell.execute_reply.started":"2023-02-19T23:53:09.031491Z"},"trusted":true},"outputs":[],"source":["# Two most expensive tickets are outliers,\n","# we will cap them at the next overall highest Price \n","df.loc[df.Ticket_nr.eq('17755'), 'Price'] = 68\n","df.loc[df.Ticket_nr.eq('17558'), 'Price'] = 68"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.051549Z","iopub.status.busy":"2023-02-19T23:53:09.051179Z","iopub.status.idle":"2023-02-19T23:53:09.101683Z","shell.execute_reply":"2023-02-19T23:53:09.099606Z","shell.execute_reply.started":"2023-02-19T23:53:09.051522Z"},"trusted":true},"outputs":[],"source":["# Create a data frame of mean prices by Pclass and Deck \n","class_deck_price = pd.DataFrame(df.groupby(['Pclass', 'Deck'])\n","                                .Price.mean().round(2)).reset_index()\n","\n","# Impute missing prices \n","# Where Deck is missing we will use the mean price by Pclass only\n","for index, row in df.loc[df.Price.isna(), \n","                         ['Pclass', 'Deck']].iterrows():\n","    if not pd.isna(row.Deck):\n","        new_price = class_deck_price.loc[\n","            (class_deck_price.Pclass.eq(row.Pclass) \n","            & class_deck_price.Deck.eq(row.Deck)), 'Price'].mean()\n","    else:\n","        new_price = class_deck_price[\n","            class_deck_price.Pclass.eq(row.Pclass)].Price.mean()\n","\n","    df.loc[[index], 'Price'] = new_price"]},{"cell_type":"markdown","metadata":{},"source":["# Analyze and impute missing Deck"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.103983Z","iopub.status.busy":"2023-02-19T23:53:09.103555Z","iopub.status.idle":"2023-02-19T23:53:09.680797Z","shell.execute_reply":"2023-02-19T23:53:09.679457Z","shell.execute_reply.started":"2023-02-19T23:53:09.103944Z"},"trusted":true},"outputs":[],"source":["# Create dictionaries with aproximative price ranges by deck \n","# concluded from previous analisys\n","first_cl = {'A': [25, 30],\n","            'B': [35, 70],\n","            'C': [30, 35],\n","            'D': [19, 25],\n","            'E': [9, 19]}\n","\n","second_cl = {'D': [13, 17],\n","             'E': [5, 9],\n","             'F': [9, 13]}\n","\n","third_cl = {'E': [8, 9],\n","            'F': [9, 21],\n","            'G': [0, 8]}\n","\n","# Create a dictionary pairing Pclass and respective price dictionary\n","class_dict = {1: first_cl,\n","              2: second_cl,\n","              3: third_cl}\n","\n","# Impute missing Deck values \n","for index, row in df.loc[df.Deck.isna(), ['Pclass', 'Price']].iterrows():\n","    for c, d in class_dict.items():\n","        if row.Pclass == c:\n","            for i, j in d.items():\n","                if max(j) > row.Price >= min(j):\n","                    df.loc[[index], 'Deck'] = i\n","\n","# Encode Deck with it's deck level number counting from the bottom\n","deck_level = {'G': 1, 'F': 2, 'E': 3, 'D': 4, 'C': 5, 'B': 6, 'A': 7}\n","\n","df.Deck = df.Deck.replace(deck_level)"]},{"cell_type":"markdown","metadata":{},"source":["# Create Escape_density\n","\n","Crowded decks could lead to jams and chaos when everybody wanted to go to the upper deck as the lifeboats were there.\n","This feature will show through which amount of people each deck passenger needed to pass to arrive on top. \n","Basically for each deck we will have a number of people equal to the summ of its own value and all the decks that are upper from it."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.682839Z","iopub.status.busy":"2023-02-19T23:53:09.682439Z","iopub.status.idle":"2023-02-19T23:53:09.696902Z","shell.execute_reply":"2023-02-19T23:53:09.695337Z","shell.execute_reply.started":"2023-02-19T23:53:09.682805Z"},"trusted":true},"outputs":[],"source":["# Analyse how many people were on each deck.\n","# Many values were imputed with aproximation,but at least we will have \n","# an aproximative crowd mass each passenger has to pass going up\n","deck_people = df.Deck.value_counts().sort_index()\n","deck_people_dic = deck_people.to_dict()\n","deck_people_dic"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.698892Z","iopub.status.busy":"2023-02-19T23:53:09.698461Z","iopub.status.idle":"2023-02-19T23:53:09.720507Z","shell.execute_reply":"2023-02-19T23:53:09.718427Z","shell.execute_reply.started":"2023-02-19T23:53:09.69886Z"},"trusted":true},"outputs":[],"source":["# Create an escape density dictionary from which we will impute data to our new feature\n","escape_density = {}\n","for i in range(1, 8):\n","    escape_density[i] = sum(deck_people_dic.values())\n","    del deck_people_dic[i]\n","    \n","escape_density"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.724033Z","iopub.status.busy":"2023-02-19T23:53:09.723466Z","iopub.status.idle":"2023-02-19T23:53:09.733636Z","shell.execute_reply":"2023-02-19T23:53:09.731434Z","shell.execute_reply.started":"2023-02-19T23:53:09.723982Z"},"trusted":true},"outputs":[],"source":["# Create Escape_density column\n","df['Escape_density'] = df.Deck.replace(escape_density)"]},{"cell_type":"markdown","metadata":{},"source":["# Create Family_size\n","\n","It will represent how big the family was"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.7368Z","iopub.status.busy":"2023-02-19T23:53:09.736318Z","iopub.status.idle":"2023-02-19T23:53:09.747055Z","shell.execute_reply":"2023-02-19T23:53:09.745776Z","shell.execute_reply.started":"2023-02-19T23:53:09.736769Z"},"trusted":true},"outputs":[],"source":["# We add together the person and his SibSp and Parch\n","df['Family_size'] = 1 + df.SibSp + df.Parch"]},{"cell_type":"markdown","metadata":{},"source":["# Create Family_survivers\n","\n","This feature can't be used for modeling as it would lead to target leakage, but by analysing it later we can separate families that could have higher surviving chance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.749013Z","iopub.status.busy":"2023-02-19T23:53:09.748608Z","iopub.status.idle":"2023-02-19T23:53:09.839529Z","shell.execute_reply":"2023-02-19T23:53:09.837756Z","shell.execute_reply.started":"2023-02-19T23:53:09.748975Z"},"trusted":true},"outputs":[],"source":["# Create full data frame for analysis\n","X = df[:X_max_index]\n","test_df = df[X_max_index:].copy()\n","full_df = pd.concat([X, y], axis=1).copy()\n","\n","# Check for families that has survivers and create a dictionary with mean value of their family survivability\n","family_survivers = full_df[['Lastname', 'Survived']].groupby('Lastname').mean().round(2).reset_index()\n","family_survivers_dict = dict(zip(family_survivers.Lastname, family_survivers.Survived))\n","\n","# Reduce the dictionary to the list of families that are both in train and test data\n","common_survivers = {}\n","for lastname, survived in family_survivers_dict.items():\n","    if lastname in list(test_df['Lastname'].unique()):\n","        common_survivers[lastname] = survived\n","\n","# Create Family_survivers feature\n","test_df['Family_survivers'] = test_df.Lastname.map(common_survivers)\n","full_df['Family_survivers'] = full_df.Lastname.map(common_survivers)\n","\n","# For the families that are not present in both train and test we will impute the overall mean value\n","test_df.Family_survivers = test_df.Family_survivers.fillna(test_df.Family_survivers.mean())\n","full_df.Family_survivers = full_df.Family_survivers.fillna(full_df.Family_survivers.mean())\n","\n","# Separate back features and target\n","y = full_df.Survived\n","\n","df = full_df.drop('Survived', axis=1)\n","df = pd.concat([df, test_df], axis=0).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.841358Z","iopub.status.busy":"2023-02-19T23:53:09.840967Z","iopub.status.idle":"2023-02-19T23:53:09.84966Z","shell.execute_reply":"2023-02-19T23:53:09.848185Z","shell.execute_reply.started":"2023-02-19T23:53:09.841328Z"},"trusted":true},"outputs":[],"source":["# Change Pclass dtype to category as it's a classification feature\n","df.Pclass = df.Pclass.astype('category')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.852296Z","iopub.status.busy":"2023-02-19T23:53:09.851707Z","iopub.status.idle":"2023-02-19T23:53:09.862827Z","shell.execute_reply":"2023-02-19T23:53:09.861171Z","shell.execute_reply.started":"2023-02-19T23:53:09.852255Z"},"trusted":true},"outputs":[],"source":["# Drop further unused columns\n","col_drop = ['Name', 'Ticket', 'Fare', 'Cabin', 'Lastname','Ticket_nr',  \n","            'Ticket_series', 'Passengers_ticket']\n","df = df.drop(col_drop, axis=1)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-02-06T22:59:43.980076Z","iopub.status.busy":"2023-02-06T22:59:43.979632Z","iopub.status.idle":"2023-02-06T22:59:43.997049Z","shell.execute_reply":"2023-02-06T22:59:43.994931Z","shell.execute_reply.started":"2023-02-06T22:59:43.98004Z"}},"source":["# Impute and encode categoricals"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.867248Z","iopub.status.busy":"2023-02-19T23:53:09.865924Z","iopub.status.idle":"2023-02-19T23:53:09.888226Z","shell.execute_reply":"2023-02-19T23:53:09.887156Z","shell.execute_reply.started":"2023-02-19T23:53:09.867194Z"},"trusted":true},"outputs":[],"source":["# List of categorical columns\n","categ_cols = list(df.select_dtypes(['object', 'category']).columns)\n","\n","# Impute categoricals with most frequent value\n","cat_imputer = SimpleImputer(strategy='most_frequent')\n","\n","df_cat = pd.DataFrame(cat_imputer.fit_transform(df[categ_cols]), \n","                      columns=df[categ_cols].columns)\n","\n","# Encode categorical\n","df_cat = pd.get_dummies(df_cat)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.00643Z","iopub.status.idle":"2023-02-06T22:59:44.006895Z","shell.execute_reply":"2023-02-06T22:59:44.006708Z","shell.execute_reply.started":"2023-02-06T22:59:44.006687Z"}},"source":["# Impute numericals"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.891529Z","iopub.status.busy":"2023-02-19T23:53:09.890633Z","iopub.status.idle":"2023-02-19T23:53:09.929914Z","shell.execute_reply":"2023-02-19T23:53:09.928618Z","shell.execute_reply.started":"2023-02-19T23:53:09.891484Z"},"trusted":true},"outputs":[],"source":["# List of numerical columns\n","num_cols = list(df.select_dtypes(['int64', 'float64']).columns)\n","\n","# Impute numericals\n","it_imp = IterativeImputer()\n","\n","df_num = pd.DataFrame(it_imp.fit_transform(df[num_cols]),\n","                      columns=df[num_cols].columns)\n","\n","# Concatenate with encoded categorical columns\n","df = pd.concat([df_cat, df_num], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Create Deck_survive_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.931909Z","iopub.status.busy":"2023-02-19T23:53:09.931566Z","iopub.status.idle":"2023-02-19T23:53:09.949349Z","shell.execute_reply":"2023-02-19T23:53:09.947607Z","shell.execute_reply.started":"2023-02-19T23:53:09.931875Z"},"trusted":true},"outputs":[],"source":["# Create a full data frame for analysis\n","X = df[:X_max_index]\n","full_df = pd.concat([X, y], axis=1)\n","\n","# Total Survived by Deck\n","deck_total_survived = full_df.groupby('Deck').Survived.sum()\n","\n","# Dictionary with deck_survive_ratio\n","deck_survive_ratio = (deck_total_survived / deck_people).to_dict()\n","\n","# Create Deck_survive_ratio\n","df['Deck_survive_ratio'] = df.Deck.map(deck_survive_ratio)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.952443Z","iopub.status.busy":"2023-02-19T23:53:09.951951Z","iopub.status.idle":"2023-02-19T23:53:09.960123Z","shell.execute_reply":"2023-02-19T23:53:09.958601Z","shell.execute_reply.started":"2023-02-19T23:53:09.952399Z"},"trusted":true},"outputs":[],"source":["# Function for kde plotting\n","def survive_chance_by(feature, xticks=None, xlim=None):\n","    survived = full_df[full_df.Survived.eq(1)]\n","    not_survived = full_df[full_df.Survived.eq(0)]\n","\n","    plt.figure(figsize=(10, 5))\n","\n","    survived[feature].plot(kind='kde', label='survived')\n","    not_survived[feature].plot(kind='kde', label='not_survived')\n","    \n","    plt.xlim(xlim)\n","    plt.xticks(xticks)\n","    plt.legend()\n","    plt.grid()\n","    plt.xlabel(feature)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.021554Z","iopub.status.idle":"2023-02-06T22:59:44.022458Z","shell.execute_reply":"2023-02-06T22:59:44.022251Z","shell.execute_reply.started":"2023-02-06T22:59:44.022224Z"}},"source":["# Create Age_group"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:09.962031Z","iopub.status.busy":"2023-02-19T23:53:09.961619Z","iopub.status.idle":"2023-02-19T23:53:10.214753Z","shell.execute_reply":"2023-02-19T23:53:10.21334Z","shell.execute_reply.started":"2023-02-19T23:53:09.96199Z"},"trusted":true},"outputs":[],"source":["# Survivers by Age\n","survive_chance_by('Age', np.arange(0, 81, 5), (0, 80))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-02-06T23:00:51.490531Z","iopub.status.busy":"2023-02-06T23:00:51.489379Z","iopub.status.idle":"2023-02-06T23:00:51.499884Z","shell.execute_reply":"2023-02-06T23:00:51.497841Z","shell.execute_reply.started":"2023-02-06T23:00:51.490487Z"}},"source":["By curves intersection points we can separate 4 age groups:\n","    \n","    1. 0-16 years old have higher survivability chance\n","    2. 16-33 years old low chance\n","    3. 33-43 years old better chance\n","    4. For the rest the chances are almost equal"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.216759Z","iopub.status.busy":"2023-02-19T23:53:10.21634Z","iopub.status.idle":"2023-02-19T23:53:10.230067Z","shell.execute_reply":"2023-02-19T23:53:10.228312Z","shell.execute_reply.started":"2023-02-19T23:53:10.216726Z"},"trusted":true},"outputs":[],"source":["df['Age_group'] = pd.cut(x=df.Age, labels=[4, 1, 3, 2],\n","                         bins=[-1, 16, 33, 43, df.Age.max()]).astype('float')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.030628Z","iopub.status.idle":"2023-02-06T22:59:44.031418Z","shell.execute_reply":"2023-02-06T22:59:44.031221Z","shell.execute_reply.started":"2023-02-06T22:59:44.031192Z"}},"source":["# Create Family_group"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.232107Z","iopub.status.busy":"2023-02-19T23:53:10.231578Z","iopub.status.idle":"2023-02-19T23:53:10.426356Z","shell.execute_reply":"2023-02-19T23:53:10.423516Z","shell.execute_reply.started":"2023-02-19T23:53:10.232061Z"},"trusted":true},"outputs":[],"source":["# Survivers by Family_size\n","survive_chance_by('Family_size', np.arange(0, 10, 1), (0, 10))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T23:00:51.508081Z","iopub.status.idle":"2023-02-06T23:00:51.508891Z","shell.execute_reply":"2023-02-06T23:00:51.508565Z","shell.execute_reply.started":"2023-02-06T23:00:51.508529Z"}},"source":["Here we can separate 3 groups:\n","\n","    1. Single persons had lower chance to survive\n","    2. 2-4 members families had higher chances, as they had some priority to safeboats with 1-2 children with them\n","    3. 5 and more members families had almost equal chances"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.429594Z","iopub.status.busy":"2023-02-19T23:53:10.429102Z","iopub.status.idle":"2023-02-19T23:53:10.439461Z","shell.execute_reply":"2023-02-19T23:53:10.438011Z","shell.execute_reply.started":"2023-02-19T23:53:10.429557Z"},"trusted":true},"outputs":[],"source":["# Create Family_group feature\n","df['Family_group'] = pd.cut(x=df.Family_size, labels=[1, 3, 2], \n","                            bins=[-1, 1, 4, df.Family_size.max()]).astype('float')"]},{"cell_type":"markdown","metadata":{},"source":["# Create Lucky_family\n","\n","To create this feature we analyse earlier created Family_survivers that used by itself would overfit the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.44194Z","iopub.status.busy":"2023-02-19T23:53:10.441431Z","iopub.status.idle":"2023-02-19T23:53:10.82128Z","shell.execute_reply":"2023-02-19T23:53:10.819744Z","shell.execute_reply.started":"2023-02-19T23:53:10.441898Z"},"trusted":true},"outputs":[],"source":["# Survivers by Family_survivers\n","survive_chance_by('Family_survivers', np.arange(0, 1.5, 0.1), (0, 1.5))"]},{"cell_type":"markdown","metadata":{},"source":["By curves intersection points we can separate 4 family groups with different chance to survive"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.823684Z","iopub.status.busy":"2023-02-19T23:53:10.823298Z","iopub.status.idle":"2023-02-19T23:53:10.836006Z","shell.execute_reply":"2023-02-19T23:53:10.8341Z","shell.execute_reply.started":"2023-02-19T23:53:10.823658Z"},"trusted":true},"outputs":[],"source":["# Create Lucky_family feature\n","df['Lucky_family'] = pd.cut(x=df.Family_survivers, labels=[2, 3, 1, 4],\n","                            bins=[-1, 0.22, 0.35, 0.49, df.Family_survivers.max()]).astype('float')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.042968Z","iopub.status.idle":"2023-02-06T22:59:44.043445Z","shell.execute_reply":"2023-02-06T22:59:44.043231Z","shell.execute_reply.started":"2023-02-06T22:59:44.04321Z"}},"source":["# Standardization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.838629Z","iopub.status.busy":"2023-02-19T23:53:10.838162Z","iopub.status.idle":"2023-02-19T23:53:10.852713Z","shell.execute_reply":"2023-02-19T23:53:10.850874Z","shell.execute_reply.started":"2023-02-19T23:53:10.838593Z"},"trusted":true},"outputs":[],"source":["# Apply np.log to normalize the skewed right Price\n","df.Price = df.Price.apply(np.log1p)\n","\n","# Standardize \n","std_scaler = StandardScaler()\n","\n","df_scaled = std_scaler.fit_transform(df)\n","df = pd.DataFrame(df_scaled, columns=df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.856193Z","iopub.status.busy":"2023-02-19T23:53:10.854871Z","iopub.status.idle":"2023-02-19T23:53:10.868812Z","shell.execute_reply":"2023-02-19T23:53:10.867652Z","shell.execute_reply.started":"2023-02-19T23:53:10.856115Z"},"trusted":true},"outputs":[],"source":["# Drop features not used for modeling\n","cols_to_drop = ['Family_survivers', 'SibSp', 'Parch', 'Family_size']\n","df = df.drop(cols_to_drop, axis=1)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-02-06T23:01:18.326265Z","iopub.status.busy":"2023-02-06T23:01:18.325562Z","iopub.status.idle":"2023-02-06T23:01:18.338288Z","shell.execute_reply":"2023-02-06T23:01:18.336437Z","shell.execute_reply.started":"2023-02-06T23:01:18.326196Z"}},"source":["# Split train and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.871627Z","iopub.status.busy":"2023-02-19T23:53:10.871072Z","iopub.status.idle":"2023-02-19T23:53:10.8815Z","shell.execute_reply":"2023-02-19T23:53:10.87988Z","shell.execute_reply.started":"2023-02-19T23:53:10.871548Z"},"trusted":true},"outputs":[],"source":["X = df[:X_max_index]\n","test_df = df[X_max_index:]"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T23:01:18.340647Z","iopub.status.idle":"2023-02-06T23:01:18.341374Z","shell.execute_reply":"2023-02-06T23:01:18.341057Z","shell.execute_reply.started":"2023-02-06T23:01:18.341023Z"}},"source":["# Processed data correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:10.896017Z","iopub.status.busy":"2023-02-19T23:53:10.895235Z","iopub.status.idle":"2023-02-19T23:53:11.114436Z","shell.execute_reply":"2023-02-19T23:53:11.111873Z","shell.execute_reply.started":"2023-02-19T23:53:10.89599Z"},"trusted":true},"outputs":[],"source":["# Concatenate into a full dataset\n","full_df = pd.concat([X, y], axis=1)\n","\n","correlation = full_df.corr()['Survived'].sort_values(ascending=False)\n","\n","# Correlation graph\n","correlation[1:].plot(kind='bar', figsize=(10,5), title='Survivability dependency')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.05844Z","iopub.status.idle":"2023-02-06T22:59:44.058839Z","shell.execute_reply":"2023-02-06T22:59:44.05867Z","shell.execute_reply.started":"2023-02-06T22:59:44.058649Z"}},"source":["# Conclusion\n","\n","On the Titanic is better to not be an usual single adult male on a lower deck and embarked from Southampton with a cheap ticket in the pocket - RIP Jack Dawson :-("]},{"cell_type":"markdown","metadata":{},"source":["# Find best features\n","This cell is commented out as it takes long time to run and the resulted final_features are shown further"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:11.11743Z","iopub.status.busy":"2023-02-19T23:53:11.116717Z","iopub.status.idle":"2023-02-19T23:53:11.122298Z","shell.execute_reply":"2023-02-19T23:53:11.121158Z","shell.execute_reply.started":"2023-02-19T23:53:11.11739Z"},"trusted":true},"outputs":[],"source":["# # Define model\n","# cat_model = CatBoostClassifier(thread_count=-1, verbose=False)\n","\n","# # Define and fit feature selector\n","# sfs = SequentialFeatureSelector(cat_model, \n","#                                 scoring='accuracy', \n","#                                 direction = 'backward')\n","# sfs.fit(X, y)\n","\n","# # List of the final features to be used for submission modeling\n","# final_features = list(sfs.get_feature_names_out())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:11.12488Z","iopub.status.busy":"2023-02-19T23:53:11.12386Z","iopub.status.idle":"2023-02-19T23:53:11.136523Z","shell.execute_reply":"2023-02-19T23:53:11.134248Z","shell.execute_reply.started":"2023-02-19T23:53:11.124837Z"},"trusted":true},"outputs":[],"source":["# From Feature selector we've got this list of final features to use\n","final_features = ['Pclass_2', 'Pclass_3', 'Sex_female', 'Title_Mr', \n","                  'Title_Mrs', 'Price', 'Deck_survive_ratio', 'Age_group',\n","                  'Family_group', 'Lucky_family']"]},{"cell_type":"markdown","metadata":{},"source":["# CatBoost grid search parameter tuning\n","This cell is commented out as it takes long time to run and the resulted parameters are shown further"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:11.139085Z","iopub.status.busy":"2023-02-19T23:53:11.138604Z","iopub.status.idle":"2023-02-19T23:53:11.149733Z","shell.execute_reply":"2023-02-19T23:53:11.14848Z","shell.execute_reply.started":"2023-02-19T23:53:11.139048Z"},"trusted":true},"outputs":[],"source":["# # Define model\n","# cat_model = CatBoostClassifier()\n","\n","# # Define parameters' grid\n","# grid = {'verbose': [False],\n","#         'thread_count': [-1],\n","#         'depth': [3, 4, 5, 6],\n","#         'iterations': [500, 1000, 2000, 3000],\n","#         'learning_rate': [0.0001, 0.001, 0.01]}\n","\n","# # Define GridSearchCV\n","# grid_cat = GridSearchCV(estimator=cat_model, param_grid=grid, cv=3, n_jobs=-1)\n","# grid_cat.fit(X[final_features], y)\n","\n","# params = grid_cat.best_params_\n","\n","# print('\\n Best Score:\\n', grid_cat.best_score_)\n","# print('\\n Best parameters:\\n', params)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:11.150911Z","iopub.status.busy":"2023-02-19T23:53:11.150623Z","iopub.status.idle":"2023-02-19T23:53:11.162447Z","shell.execute_reply":"2023-02-19T23:53:11.160647Z","shell.execute_reply.started":"2023-02-19T23:53:11.150881Z"},"trusted":true},"outputs":[],"source":["# Best parameters\n","params = {'verbose': False,\n","          'thread_count': -1,\n","          'depth': 4, \n","          'iterations': 1000, \n","          'learning_rate': 0.0005}"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.061203Z","iopub.status.idle":"2023-02-06T22:59:44.061641Z","shell.execute_reply":"2023-02-06T22:59:44.061463Z","shell.execute_reply.started":"2023-02-06T22:59:44.061442Z"}},"source":["# Final model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:11.164431Z","iopub.status.busy":"2023-02-19T23:53:11.164063Z","iopub.status.idle":"2023-02-19T23:53:14.558038Z","shell.execute_reply":"2023-02-19T23:53:14.556461Z","shell.execute_reply.started":"2023-02-19T23:53:11.164395Z"},"trusted":true},"outputs":[],"source":["# Define and fit the model\n","cat_model = CatBoostClassifier(**params)\n","cat_model.fit(X[final_features], y)\n","\n","# Check accuracy and features importance\n","cat_rmses = cross_val_score(cat_model, X[final_features], y, cv=5)\n","\n","print(pd.Series(cat_rmses).describe())\n","print('\\n', cat_model.get_feature_importance(prettified=True))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2023-02-06T22:59:44.065525Z","iopub.status.idle":"2023-02-06T22:59:44.066348Z","shell.execute_reply":"2023-02-06T22:59:44.066094Z","shell.execute_reply.started":"2023-02-06T22:59:44.066054Z"}},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T23:53:14.560252Z","iopub.status.busy":"2023-02-19T23:53:14.559856Z","iopub.status.idle":"2023-02-19T23:53:14.573116Z","shell.execute_reply":"2023-02-19T23:53:14.571275Z","shell.execute_reply.started":"2023-02-19T23:53:14.560219Z"},"trusted":true},"outputs":[],"source":["# Make predictions which we will submit.\n","test_preds = cat_model.predict(test_df[final_features])\n","\n","# Save predictions in the format used for competition scoring\n","output = pd.DataFrame({'PassengerId': test_pass_id,\n","                       'Survived': test_preds})\n","output.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Data_science","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"vscode":{"interpreter":{"hash":"da9e3db53a8844e9f893dbe2efe1428093505ef7ff77c54c39804fa2ef597529"}}},"nbformat":4,"nbformat_minor":4}
